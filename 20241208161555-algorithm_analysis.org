:PROPERTIES:
:ID:       4e7072fb-fcb2-482b-9a0b-3871340bcc9b
:END:
#+title: Algorithm Analysis
#+tags: [[id:5a2a8786-c4c9-44b8-9868-e7633631aebe][DSA]]

* The RAM Model of Computation

Machine-independent algorithm design depends upon a hypothetical computer called the /Random Access Machine/ or RAM.

- Each /simple/ operation (+, *, -, =, if, call) takes exactly one time step.
- Loops and subroutines are /not/ considered simple operations. They are a composition of many single-step operations.
- Each memory access takes exactly one time step.

Under the RAM model, we measure run time by counting up the number of steps an algorithm takes on a given problem instance.

** Best, Worst, and Average-Case Complexity

- Worst-case complexity is the function defined by the maximum number of steps taken in any instance of size ~n~.
- Best-case complexity is the function defined by the minimum number of steps taken in any instance of size ~n~.
- Average-case complexity is the function defined by the average number of steps over all instances of size ~n~.

* Big O Notation

The Big O notation simplifies algorithm analysis by ignoring levels of detail that do not impact our comparison of algorithms.

* Growth Rates and Dominance Relations

Although we are not that concerned over constant factors, the growth rate of functions can give us an idea on determining if a given algorithm is appropriate for a problem of a given size.


** Dominance Relations

A faster-growing functions /dominates/ a slower-growing one.

Algorithms listed in increasing dominance:

- *Constant functions*, ~f(n) = 1~
  - There is no dependence on the parameter ~n~
  - Examples are adding two numbers, or printing a string.
- *Logarithmic functions*, ~f(n) = log n~
  - Functions grow quite slowly as ~n~ gets big.
  - /Binary search/
- *Linear functions*, ~f(n) = n~
  - Such functions measure the cost of looking at each item once (or twice, or ten times) in an n-element array
- *Superlinear functions*, ~f(n) = n log n~
  - /Quicksort/, /Mergesort/
- *Quadratic functions*, ~f(n) = n^2~
  - Measure the cost of looking at most or all /pairs/ of items in an n-element universe.
  - /Insertion sort/, /selection sort/
- *Cubic functions*, ~f(n) = n^3~
  - Enumerate through all /triples/ of items in an n-elemennt universe
  - Arise in certain /dynamic programming/ algorithms
- *Exponential functions*, ~f(n) = c^n~ for a given constant ~c > 1~
  - Arises when enumerating all subsets of n items
- *Factorial functions*, ~f(n) = n!~
  - Arise when generating all permutations or orderings of n items.

* Working with the Big O

** Adding Functions

We usually drop the complexity of the smaller function if atleast half the bulk of ~f(n) + g(n)~ comes from the larger value.

~O(f(n)) + O(g(n)) -> O(max(f(n), g(n)))~

** Multiplying Functions

Multiplying a function to a constant cannot affect its asymptotic behavior. So ~O(c * f(n)) -> O(f(n))~

On the other hand, when two functions are increasing, ~O(f(n)) * O(g(n)) -> O(f(n) * g(n))~

* Reasoning About Efficiency

** Selection Sort

Repeatedly identifies the smallest remaining unsorted element and puts it at the end of the sorted portion of the array.

#+begin_src c
  selection_sort(int s[], int n) {
      int i, j; /* counters */
      int min;  /* index of minimum */

      for (i = 0; i < n; i++) {
          min = i;
          for (j = i + 1; j < n; j++)
              if (s[j] < s[min]) min = j;
          swap(&s[i], &s[min]);
      }
  }

#+end_src

** Insertion Sort

Worst case running time follows from multiplying the largest number of times each nested loop can iterate.

#+begin_src c
  for (i = 1; i < n; i++) {
      j = i;
      while ((j > 0) && (s[j] < s[j - 1])) {
          swap(&s[j], &s[j - 1]);
          j = j - 1;
      }
  }
#+end_src


In this case, the inner /while/ loop interates,
